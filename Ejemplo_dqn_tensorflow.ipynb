{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0eb5635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerias\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from IPython.display import clear_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c38c2068",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\",render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62b2c5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00279675,  1.4215324 , -0.28330046,  0.47166222,  0.00324757,\n",
       "         0.06417184,  0.        ,  0.        ], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a83f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 300\n",
      "Total Reward: -22528.48669500898\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
     ]
    }
   ],
   "source": [
    "# Play without training\n",
    "env.reset()\n",
    "steps = 0\n",
    "G = 0\n",
    "done = False\n",
    "\n",
    "#while not done:\n",
    "for i in range(300):\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, truncated,info = env.step(action)\n",
    "    \n",
    "    G += reward\n",
    "    steps += 1\n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(steps))\n",
    "print(\"Total Reward: {}\".format(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47d614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c16f080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros del agente y del entorno\n",
    "EPOCHS = 20000  # Número máximo de episodios de entrenamiento\n",
    "MAX_STEPS = 1000  # Máximo de pasos por episodio\n",
    "BATCH_SIZE = 64  # Tamaño del lote para el entrenamiento\n",
    "GAMMA = 0.99  # Factor de descuento para recompensas futuras\n",
    "LEARNING_RATE = 0.001  # Tasa de aprendizaje de la red\n",
    "EPSILON_DECAY = 0.995  # Factor de decaimiento de epsilon (exploración)\n",
    "EPSILON_MIN = 0.01  # Valor mínimo de epsilon\n",
    "EPSILON = 1.0  # Valor inicial de epsilon (exploración total)\n",
    "UPDATE_TARGET_NETWORK = 1000  # Frecuencia de actualización de la red objetivo\n",
    "UPDATE_AFTER_ACTIONS = 4  # Frecuencia de entrenamiento de la red principal\n",
    "MAX_MEMORY_LENGTH = 100000  # Capacidad máxima del buffer de experiencia\n",
    "\n",
    "# Crear el entorno de LunarLander\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"none\")\n",
    "\n",
    "# Definir la arquitectura de la red neuronal Q (Q-network)\n",
    "def build_model(state_shape, action_size):\n",
    "    # Red neuronal simple con dos capas ocultas\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=state_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(action_size, activation='linear')  # Salida: un valor Q por acción\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Inicializar las redes principal y objetivo\n",
    "state_shape = (env.observation_space.shape[0],)  # Dimensión del estado\n",
    "action_size = env.action_space.n  # Número de acciones posibles\n",
    "model = build_model(state_shape, action_size)  # Red principal\n",
    "model_target = build_model(state_shape, action_size)  # Red objetivo\n",
    "model_target.set_weights(model.get_weights())  # Sincronizar pesos al inicio\n",
    "\n",
    "# Inicializar el buffer de experiencia (replay buffer)\n",
    "replay_buffer = deque(maxlen=MAX_MEMORY_LENGTH)\n",
    "\n",
    "# Variables para el entrenamiento\n",
    "episode_count = 0  # Contador de episodios\n",
    "frame_count = 0  # Contador de pasos totales\n",
    "episode_reward_history = []  # Historial de recompensas por episodio\n",
    "\n",
    "# Bucle principal de entrenamiento\n",
    "while episode_count < EPOCHS:\n",
    "    state, info = env.reset()  # Reiniciar el entorno y obtener el estado inicial\n",
    "    state = np.array(state)\n",
    "    episode_reward = 0  # Recompensa acumulada en el episodio\n",
    "    done = False\n",
    "\n",
    "    # Un episodio completo\n",
    "    for timestep in range(MAX_STEPS):\n",
    "        # Selección de acción: política epsilon-greedy\n",
    "        # Durante las primeras iteraciones, forzamos exploración; luego, exploramos con probabilidad epsilon\n",
    "        if frame_count < EPSILON_DECAY * EPOCHS or EPSILON > np.random.rand(1)[0]:\n",
    "            action = np.random.choice(action_size)  # Exploración: acción aleatoria\n",
    "        else:\n",
    "            # Explotación: elegir la mejor acción según la red principal\n",
    "            state_tensor = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)  # Añadir dimensión batch\n",
    "            action_values = model(state_tensor, training=False)\n",
    "            action = tf.argmax(action_values[0]).numpy()\n",
    "\n",
    "        # Ejecutar la acción en el entorno\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        next_state = np.array(next_state)\n",
    "        episode_reward += reward  # Acumular recompensa\n",
    "\n",
    "        # Guardar la experiencia en el buffer de repetición\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "        state = next_state  # Actualizar el estado actual\n",
    "        frame_count += 1  # Incrementar el contador de pasos\n",
    "\n",
    "        # Entrenar la red principal si hay suficientes muestras y toca entrenar\n",
    "        if len(replay_buffer) > BATCH_SIZE and frame_count % UPDATE_AFTER_ACTIONS == 0:\n",
    "            # Seleccionar un lote aleatorio del buffer\n",
    "            batch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = np.array(states)\n",
    "            next_states = np.array(next_states)\n",
    "            actions = np.array(actions)\n",
    "            rewards = np.array(rewards)\n",
    "            dones = np.array(dones, dtype=float)\n",
    "\n",
    "            # Calcular los valores Q futuros usando la red objetivo\n",
    "            future_rewards = model_target.predict(next_states)\n",
    "            # Q-valor objetivo: recompensa inmediata + valor futuro descontado (si no ha terminado)\n",
    "            updated_q_values = rewards + GAMMA * np.max(future_rewards, axis=1) * (1 - dones)\n",
    "\n",
    "            # Calcular los Q-valores actuales para las acciones tomadas\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_values = model(states)\n",
    "                action_mask = tf.one_hot(actions, action_size)  # Enmascarar la acción tomada\n",
    "                q_action = tf.reduce_sum(q_values * action_mask, axis=1)\n",
    "\n",
    "                # Calcular la pérdida (error cuadrático medio)\n",
    "                loss = tf.reduce_mean(tf.square(updated_q_values - q_action))\n",
    "\n",
    "            # Aplicar los gradientes para actualizar la red principal\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            # Mostrar información de entrenamiento cada 100 pasos\n",
    "            if frame_count % 100 == 0:\n",
    "                print(f\"Frame {frame_count} | Loss: {loss.numpy():.4f} | Epsilon: {EPSILON:.2f}\")\n",
    "\n",
    "        # Actualizar la red objetivo cada cierto número de pasos\n",
    "        if frame_count % UPDATE_TARGET_NETWORK == 0:\n",
    "            model_target.set_weights(model.get_weights())\n",
    "\n",
    "        # Reducir epsilon (menos exploración con el tiempo, pero nunca menor que EPSILON_MIN)\n",
    "        EPSILON = max(EPSILON * EPSILON_DECAY, EPSILON_MIN)\n",
    "\n",
    "        # Si el episodio termina (por done o truncado), salir del bucle\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    # Guardar la recompensa del episodio y calcular la media móvil\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        episode_reward_history.pop(0)\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "    # Mostrar información cada 100 episodios\n",
    "    if episode_count % 100 == 0:\n",
    "        print(f\"Episode {episode_count} | Episode Reward: {episode_reward} | Running Reward: {running_reward}\")\n",
    "\n",
    "    # Comprobar si la tarea se considera resuelta\n",
    "    if running_reward > 200:\n",
    "        print(f\"Solved at episode {episode_count}!\")\n",
    "        break\n",
    "\n",
    "    episode_count += 1  # Siguiente episodio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
