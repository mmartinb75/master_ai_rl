{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "713d6669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab44baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[box2d] in /Users/mmartin/opt/anaconda3/envs/rl_course/lib/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/mmartin/opt/anaconda3/envs/rl_course/lib/python3.12/site-packages (from gymnasium[box2d]) (2.1.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/mmartin/opt/anaconda3/envs/rl_course/lib/python3.12/site-packages (from gymnasium[box2d]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/mmartin/opt/anaconda3/envs/rl_course/lib/python3.12/site-packages (from gymnasium[box2d]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/mmartin/opt/anaconda3/envs/rl_course/lib/python3.12/site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /Users/mmartin/opt/anaconda3/envs/rl_course/lib/python3.12/site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /Users/mmartin/opt/anaconda3/envs/rl_course/lib/python3.12/site-packages (from gymnasium[box2d]) (2.6.1)\n",
      "Requirement already satisfied: swig==4.* in /Users/mmartin/opt/anaconda3/envs/rl_course/lib/python3.12/site-packages (from gymnasium[box2d]) (4.3.1)\n"
     ]
    }
   ],
   "source": [
    "#!pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a237e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f07c244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 150\n",
      "Total Reward: 0\n"
     ]
    }
   ],
   "source": [
    "# 1.- Vamos a ejecutar una estrategia con acciones escogidas al azar.\n",
    "#env.s = 328  # set environment to illustration's state\n",
    "import time\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "G = 0\n",
    "frames = [] # for animation\n",
    "done = False\n",
    "observation, info = env.reset(seed=42)\n",
    "#while not done:\n",
    "for i in range(150):\n",
    "\n",
    "    #frame = env.render()\n",
    "\n",
    "    # Show frame inline in Jupyter\n",
    "    #plt.imshow(frame)\n",
    "    #plt.axis(\"off\")\n",
    "    #display(plt.gcf())              # Show current figure\n",
    "    #clear_output(wait=True)        # Clear previous output\n",
    "    action = env.action_space.sample() # muestreamos una acción al azar\n",
    "    state, reward, done, truncated,info = env.step(action) # ejecutamos dicha acción en nuestro entorno.\n",
    "    \n",
    "    G += reward\n",
    "    epochs += 1\n",
    "\n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Total Reward: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3999f033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x34265d6d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "#env = gym.make(\"LunarLander-v3\")\n",
    "vec_env = make_vec_env(\"LunarLander-v3\", n_envs=4)\n",
    "model = PPO(\"MlpPolicy\", vec_env, verbose=0)\n",
    "model.learn(total_timesteps=125000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be390a08",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 21\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5000\u001b[39m):\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#frame = env.render()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#display(plt.gcf())              # Show current figure\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m#clear_output(wait=True)        # Clear previous output\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(state) \u001b[38;5;66;03m# muestreamos una acción al azar\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     state, reward, done, truncated,info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action) \u001b[38;5;66;03m# ejecutamos dicha acción en nuestro entorno.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     G \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     24\u001b[0m     epochs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rl_course/lib/python3.12/site-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rl_course/lib/python3.12/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rl_course/lib/python3.12/site-packages/gymnasium/core.py:327\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rl_course/lib/python3.12/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rl_course/lib/python3.12/site-packages/gymnasium/envs/box2d/lunar_lander.py:665\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    662\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 665\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(state, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rl_course/lib/python3.12/site-packages/gymnasium/envs/box2d/lunar_lander.py:778\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    777\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 778\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_fps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    779\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "#env.s = 328  # set environment to illustration's state\n",
    "import time\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "G = 0\n",
    "frames = [] # for animation\n",
    "done = False\n",
    "observation, info = env.reset(seed=42)\n",
    "#while not done:\n",
    "for i in range(5000):\n",
    "\n",
    "    #frame = env.render()\n",
    "\n",
    "    # Show frame inline in Jupyter\n",
    "    #plt.imshow(frame)\n",
    "    #plt.axis(\"off\")\n",
    "    #display(plt.gcf())              # Show current figure\n",
    "    #clear_output(wait=True)        # Clear previous output\n",
    "    action, _states = model.predict(state) # muestreamos una acción al azar\n",
    "    state, reward, done, truncated,info = env.step(action) # ejecutamos dicha acción en nuestro entorno.\n",
    "    \n",
    "    G += reward\n",
    "    epochs += 1\n",
    "\n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Total Reward: {}\".format(penalties))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
